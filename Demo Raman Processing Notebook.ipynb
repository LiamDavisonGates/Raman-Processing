{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "progressive-point",
   "metadata": {},
   "source": [
    "<h1>This notebook is to demo the spectral processing module</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the spectral processing module.\n",
    "from SpectralProcessing import RamanProcessing as rp\n",
    "\n",
    "# Along with some other modles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we need to creat a list of the files we want to process.\n",
    "file_paths  =  ['Data\\Fed_Sample_1.txt',\n",
    "                'Data\\Fed_Sample_2.txt',\n",
    "                'Data\\Fed_Sample_3.txt',\n",
    "                'Data\\Starved_Sample_1.txt',\n",
    "                'Data\\Starved_Sample_2.txt',\n",
    "                'Data\\Starved_Sample_3.txt']\n",
    "\n",
    "# We will also need a list of ID's for each file.\n",
    "sample_type =  ['Fed',\n",
    "                'Fed',\n",
    "                'Fed',\n",
    "                'Starved',\n",
    "                'Starved',\n",
    "                'Starved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the list of files we will call 'readArrayFromFile()' to split each spectra in the\n",
    "# files into an array of samples agains wavenumbers. This function also outputs a 1-D vector\n",
    "# of wavenumbers and a 1-D vector of sample ID's corrisponding to the list we assinged earlyer.\n",
    "# Note: this function is only able to convert .txt files of map grid spectral collectrion from\n",
    "# the WiER 2 software pakage.\n",
    "WN, array, sample_ID = rp.readArrayFromFile(file_paths, sample_type)\n",
    "\n",
    "# From here we read the array of spectras into a data frame. The function needs a string to\n",
    "# specify the name of the first column in the datframe. Each spectra in split into a 1-D \n",
    "# vector and stored in a single cell in the data frame.\n",
    "df = rp.readArrayToDataFrame(array, 'Raw_array')\n",
    "\n",
    "# Here we used the sample_ID vector to add a column with corrisponding labes for each spectra.\n",
    "df['Sample_type'] = sample_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-lindsay",
   "metadata": {},
   "source": [
    "**Now we have a data frame with the raw spectras in one column and their corisponding sample ID in another column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the data frame to see our nely organised data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access a column of spectras we can stack them into an array.\n",
    "spectras = np.stack(df['Raw_array'])\n",
    "\n",
    "# This new array can be plotted.\n",
    "plt.rcParams['figure.figsize'] = [18,10]\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 24}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "plt.plot(WN, np.transpose(spectras))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-approach",
   "metadata": {},
   "source": [
    "**This module uses the dataframe as a history of the spectra processing. The function 'addColumnToDataFrame()' allows an array to be added to the exsiting df (note: 'readArrayToDataFrame()' creats a new data frame each time so you cant add coluns using it).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets smooth the data. The 'smooth()' function takes a column from our dataframe and\n",
    "# applies a smoothing algorithum to the data. In this case we will used a fast fourior\n",
    "# transform (FFT) to smooth the data.\n",
    "smoothed_array = rp.smooth(df['Raw_array'], method = 'FFT', fourior_values = 250)\n",
    "\n",
    "# Once tyhe data is porcess we can add it to the data frame. The first argument is the data\n",
    "# frame we are using, the second is the array we want to add and the third is the name for\n",
    "# the new column.\n",
    "df = rp.addColumnToDataFrame(df, smoothed_array, 'Smoothed_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have two columns for spectras.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now stack and plot this new column.\n",
    "spectras_smoothed = np.stack(df['Smoothed_array'])\n",
    "\n",
    "plt.plot(WN, np.transpose(spectras_smoothed))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Smoothed Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-cinema",
   "metadata": {},
   "source": [
    "**Using this method we can creat a whole processing pipeline with whatever steps we want in whatever order we want.**\n",
    "\n",
    "**Lets take a look at a more complicated processing pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-glory",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets take our smoothed data and normalise, balseline correct, despike and normalise (again).\n",
    "df = rp.addColumnToDataFrame(df,\n",
    "                             rp.normalise(df['Smoothed_array'],\n",
    "                                          method = 'interp_area',\n",
    "                                          normalisation_indexs = (895,901)),\n",
    "                             'Normalized_array')\n",
    "\n",
    "df = rp.addColumnToDataFrame(df,\n",
    "                             rp.baselineCorrection(df['Normalized_array'],\n",
    "                                                   method = 'ALS',\n",
    "                                                   lam=10**5),\n",
    "                             'Baseline_corrected_array')\n",
    "\n",
    "df = rp.addColumnToDataFrame(df,\n",
    "                             rp.removeCosmicRaySpikes(df['Baseline_corrected_array'],\n",
    "                                                      threshold = 5),\n",
    "                             'Despiked_array')\n",
    "\n",
    "df = rp.addColumnToDataFrame(df,\n",
    "                             rp.normalise(df['Despiked_array'],\n",
    "                                          method = 'interp_area',\n",
    "                                          normalisation_indexs = (895,901)),\n",
    "                             'Baseline_corrected_normalized_array')\n",
    "\n",
    "# Now we can plot the results of each stage in theis pipeline.\n",
    "plt.plot(WN,np.transpose(np.stack(df['Normalized_array'])))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Normalized Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()\n",
    "plt.plot(WN,np.transpose(np.stack(df['Baseline_corrected_array'])))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Baseline Corrected Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()\n",
    "plt.plot(WN,np.transpose(np.stack(df['Despiked_array'])))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Despiked Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()\n",
    "plt.plot(WN,np.transpose(np.stack(df['Baseline_corrected_normalized_array'])))\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.title('Normalized Baseline Corrected Raman Spectras')\n",
    "plt.xlabel('Wavenumbers (CM$^{-1}$)')\n",
    "plt.ylabel('Intencity (AU)') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-bread",
   "metadata": {},
   "source": [
    "**This whole pipeline can be quickly implimented by passing the file name list and sample ID list to the 'quickProcess()' function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also assess the effectiness of each step by calculating the signal to noise ratio.\n",
    "rp.signalToNoiseOfDataframe(df)\n",
    "\n",
    "# It should be noted that the signal to noise calculation here uses the standard devaition\n",
    "# as the noise and the squareroot of the mean as the siganl. Because of this squareroot it is\n",
    "# bias towards smaller values so and data that is not normalised to 1.0 can not be compaiered\n",
    "# accuratly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the spectras seperated out by class using the 'plotSpectraByClass()' function.\n",
    "# This function takes five arguments; the data frame to be used , the x axis, the column to plot,\n",
    "# the spectra ids for the classes you want to plot and the spetcra ids coulmn which is the coulmn\n",
    "# the spectra ids are stored in.\n",
    "rp.plotSpectraByClass(df,\n",
    "                      WN,\n",
    "                      'Baseline_corrected_normalized_array',\n",
    "                      set(df['Sample_type']),# The set gives us all uniqe entrys in this column.\n",
    "                      'Sample_type')\n",
    "\n",
    "# A simpliar function can also plot the principal component alysis for the given spectras.\n",
    "rp.plotPCAByClass(df,\n",
    "                  'Baseline_corrected_normalized_array',\n",
    "                  set(df['Sample_type']),# The set gives us all uniqe entrys in this column.\n",
    "                  'Sample_type',\n",
    "                  principal_components=10,\n",
    "                  PCs_plot=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-playback",
   "metadata": {},
   "source": [
    "**The module also comes with functions to utalize machine learning (ML) for spectral analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning are supresed as scikit learn can clog up the output when running multiple\n",
    "# repeats of ML modles.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Lets apply some ML models to our data. The function 'applyMachineLearingPredictors()'\n",
    "# takes two positional arguments; fist is the column we want to test and the second is\n",
    "# the sample ID's. A full list of the peramiter for each function can be found in the\n",
    "# documentation.\n",
    "CV_Train, CV_Test = rp.applyMachineLearingPredictors(df['Baseline_corrected_normalized_array'],\n",
    "                                                     df['Sample_type'],\n",
    "                                                     principal_components=10,\n",
    "                                                     CV=10,\n",
    "                                                     test_size=0.33)\n",
    "\n",
    "# The output is two dictionarys of cross valiudation values for both a traing data set\n",
    "# and test data set.\n",
    "print('Cross-validation results for the training data set')\n",
    "pprint(CV_Train)\n",
    "print('')\n",
    "print('Cross-validation results for the test data set')\n",
    "pprint(CV_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results can be better visulised via the 'dispayCVResults()' function.\n",
    "rp.dispayCVResults(CV_Train)\n",
    "rp.dispayCVResults(CV_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-breach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
